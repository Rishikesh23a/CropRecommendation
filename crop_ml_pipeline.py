
"""crop_ml_pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/179hLydBtIzBNTzNojDQO2uEgMVQ6ugqu
"""

import os
import numpy as np
import pandas as pd
import joblib
import matplotlib.pyplot as plt
import seaborn as sns   # seaborn only for nicer tables/heatmaps (optional)
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.dummy import DummyClassifier
from collections import Counter
import warnings
warnings.filterwarnings("ignore")

DATA_PATH = "final_datasetCrop_recommendationV2 (1).xlsx"   # <--- change if needed
TARGET_COL = "label"
RANDOM_STATE = 42
TEST_SIZE = 0.20
OUT_DIR = "ml_artifacts"
os.makedirs(OUT_DIR, exist_ok=True)

df = pd.read_excel(DATA_PATH)
print("Loaded:", DATA_PATH)
print("Shape:", df.shape)
print("\nColumns:", df.columns.tolist())

# Basic preview
print("\nFirst rows:")
print(df.head())


print("\nData types:")
print(df.dtypes)

print("\nMissing values per column:")
print(df.isnull().sum())

# If there are columns like 'Unnamed', drop those (common from Excel)
unnamed_cols = [c for c in df.columns if str(c).lower().startswith("unnamed")]
if unnamed_cols:
    print("\nDropping unnamed cols:", unnamed_cols)
    df = df.drop(columns=unnamed_cols)

# Summary statistics
print("\nNumeric summary:")
print(df.describe().T)

# Show class distribution
print("\nTarget value counts:")
print(df[TARGET_COL].value_counts().sort_values(ascending=False).head(20))
print("Total classes:", df[TARGET_COL].nunique())

plt.figure(figsize=(10,4))
top_counts = df[TARGET_COL].value_counts().nlargest(20)
plt.bar(top_counts.index.astype(str), top_counts.values)
plt.xticks(rotation=45, ha='right')
plt.title("Top 20 label counts")
plt.tight_layout()
plt.savefig(os.path.join(OUT_DIR,"label_top20_counts.png"))
plt.close()

# Detect numeric feature columns and categorical columns
all_cols = df.columns.tolist()
feature_cols = [c for c in all_cols if c != TARGET_COL]
numeric_cols = df[feature_cols].select_dtypes(include=[np.number]).columns.tolist()
categorical_cols = [c for c in feature_cols if c not in numeric_cols]

print("\nDetected numeric features:", numeric_cols)
print("Detected categorical features:", categorical_cols)

# If there are categorical features, we'll one-hot encode them; else only scale numeric.
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])

from sklearn.preprocessing import OneHotEncoder
# Handle both old and new scikit-learn versions
try:
    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
    ])
except TypeError:
    # Fallback for older scikit-learn (<1.2)
    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))
    ])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_cols),
        ('cat', categorical_transformer, categorical_cols)
    ], remainder='drop'
)

# Prepare X, y
X = df[feature_cols].copy()
y = df[TARGET_COL].copy()

# Label encode target
le = LabelEncoder()
y_enc = le.fit_transform(y)
print("\nClasses:", le.classes_)
# Save mapping
mapping = dict(zip(le.classes_, le.transform(le.classes_)))
pd.Series(mapping).to_frame("label_index").to_csv(os.path.join(OUT_DIR,"label_mapping.csv"))

# If any class has only one sample, stratify will fail. Check and handle:
class_counts = pd.Series(y_enc).value_counts()
if class_counts.min() < 2:
    print("Warning: some classes have fewer than 2 samples; will NOT stratify split.")
    stratify_arg = None
else:
    stratify_arg = y_enc

X_train, X_test, y_train, y_test = train_test_split(X, y_enc, test_size=TEST_SIZE,
                                                    random_state=RANDOM_STATE,
                                                    stratify=stratify_arg)
print("\nTrain shape:", X_train.shape, "Test shape:", X_test.shape)
print("Train class distribution:", Counter(y_train))
print("Test class distribution:", Counter(y_test))

# Fit preprocessor on training data
X_train_pre = preprocessor.fit_transform(X_train)
X_test_pre = preprocessor.transform(X_test)

# Save preprocessor
joblib.dump(preprocessor, os.path.join(OUT_DIR,"preprocessor.pkl"))
print("Saved preprocessor to", os.path.join(OUT_DIR,"preprocessor.pkl"))

models = {
    "Dummy": DummyClassifier(strategy="most_frequent"),
    "LogisticRegression": LogisticRegression(max_iter=2000, random_state=RANDOM_STATE),
    "DecisionTree": DecisionTreeClassifier(random_state=RANDOM_STATE),
    "RandomForest": RandomForestClassifier(n_estimators=200, random_state=RANDOM_STATE, n_jobs=-1),
    "KNN": KNeighborsClassifier(),
    "SVM": SVC(probability=True, random_state=RANDOM_STATE)
}

# Add XGBoost if available (optional)
try:
    from xgboost import XGBClassifier
    models["XGBoost"] = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=RANDOM_STATE)
    print("XGBoost available and added.")
except Exception:
    print("XGBoost not available in environment; skipping.")

results = []
reports = {}
conf_mats = {}

for name, clf in models.items():
    print("\nTraining:", name)
    clf.fit(X_train_pre, y_train)
    y_pred = clf.predict(X_test_pre)
    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred, average='weighted', zero_division=0)
    rec = recall_score(y_test, y_pred, average='weighted', zero_division=0)
    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
    results.append((name, acc, prec, rec, f1))
    reports[name] = classification_report(y_test, y_pred, target_names=le.classes_, zero_division=0)
    conf_mats[name] = confusion_matrix(y_test, y_pred)
    print(f"{name} — Accuracy: {acc:.4f}, F1-weighted: {f1:.4f}")

# Save results DataFrame
results_df = pd.DataFrame(results, columns=["model","accuracy","precision","recall","f1"]).sort_values("accuracy", ascending=False)
results_df.to_csv(os.path.join(OUT_DIR,"model_comparison.csv"), index=False)
print("\nModel comparison saved to", os.path.join(OUT_DIR,"model_comparison.csv"))
print(results_df)


# Plot comparison
plt.figure(figsize=(9,4))
plt.bar(results_df['model'], results_df['accuracy'], alpha=0.8)
plt.ylim(0,1)
plt.ylabel("Accuracy")
plt.title("Model comparison (test accuracy)")
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.savefig(os.path.join(OUT_DIR,"model_comparison_accuracy.png"))
plt.close()

# Save classification reports and confusion matrices
for name, rep in reports.items():
    with open(os.path.join(OUT_DIR, f"{name}_classification_report.txt"), "w") as f:
        f.write(rep)
    cm = conf_mats[name]
    # plot confusion matrix heatmap for the top models only (to keep visuals readable)
    plt.figure(figsize=(8,6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f"{name} - Confusion Matrix")
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.tight_layout()
    plt.savefig(os.path.join(OUT_DIR, f"{name}_confusion_matrix.png"))
    plt.close()

print("\nPerforming 5-fold cross-validation on top models for robustness check...")
top_models = results_df['model'].head(3).tolist()  # top 3
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)
cv_scores = {}
for m in top_models:
    clf = models[m]
    try:
        scores = cross_val_score(clf, preprocessor.transform(X), y_enc, cv=cv, scoring='accuracy', n_jobs=-1)
        cv_scores[m] = (scores.mean(), scores.std())
        print(f"{m}: CV mean acc = {scores.mean():.4f} ± {scores.std():.4f}")
    except Exception as e:
        print("CV failed for", m, ":", e)

best_model_name = results_df.iloc[0]['model']
best_model = models[best_model_name]
print("\nSelected best model:", best_model_name)

# Save model bundle (pipeline alternative: save preprocessor + model separately)
joblib.dump({'model': best_model, 'label_encoder': le, 'preprocessor': preprocessor, 'features': feature_cols},
            os.path.join(OUT_DIR, f"best_model_{best_model_name}.pkl"))
print("Saved best model bundle to", os.path.join(OUT_DIR, f"best_model_{best_model_name}.pkl"))

# -------------------------

def predict_from_dict(input_dict):
    """
    input_dict: dict of feature->value for features in feature_cols (strings)
    returns: predicted label (string)
    """
    # create single-row dataframe with same columns as X
    row = pd.DataFrame([input_dict])
    proc = preprocessor.transform(row)
    pred_idx = best_model.predict(proc)
    return le.inverse_transform(pred_idx)[0]

# Streamlit app (saved to file)
streamlit_code = f'''
# Streamlit Crop Recommendation demo
import streamlit as st
import joblib, numpy as np, pandas as pd
st.title("Crop Recommendation (Demo)")

bundle = joblib.load("{os.path.join(OUT_DIR, f'best_model_{best_model_name}.pkl')}")
model = bundle['model']
le = bundle['label_encoder']
preprocessor = bundle['preprocessor']
features = bundle['features']

st.sidebar.header("Input sensor values")
inputs = {{}}
for feat in features:
    inputs[feat] = st.sidebar.number_input(str(feat), value=0.0)

if st.button("Predict"):
    df = pd.DataFrame([inputs])
    try:
        X_proc = preprocessor.transform(df)
    except Exception as e:
        st.error("Preprocessor error: " + str(e))
    pred_idx = model.predict(X_proc)
    pred_label = le.inverse_transform(pred_idx)[0]
    st.success(f"Recommended crop: {{pred_label}}")
'''

with open(os.path.join(OUT_DIR,"streamlit_app.py"), "w") as f:
    f.write(streamlit_code)
print("Saved Streamlit app to", os.path.join(OUT_DIR,"streamlit_app.py"))

# Flask API stub (saved to file)
flask_code = f'''
# Flask API for Crop Recommendation
from flask import Flask, request, jsonify
import joblib, numpy as np, pandas as pd

app = Flask(__name__)
bundle = joblib.load("{os.path.join(OUT_DIR, f'best_model_{best_model_name}.pkl')}")
model = bundle['model']
le = bundle['label_encoder']
preprocessor = bundle['preprocessor']
features = bundle['features']

@app.route('/predict', methods=['POST'])
def predict():
    data = request.json
    # expects JSON with feature:value pairs
    df = pd.DataFrame([{{f: data.get(f, 0) for f in features}}])
    X_proc = preprocessor.transform(df)
    pred_idx = model.predict(X_proc)
    pred_label = le.inverse_transform(pred_idx)[0]
    return jsonify({{'recommended_crop': pred_label}})

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=True)
'''
with open(os.path.join(OUT_DIR,"flask_app.py"), "w") as f:
    f.write(flask_code)
print("Saved Flask app to", os.path.join(OUT_DIR,"flask_app.py"))

print("\nAll done. Artifacts saved to", OUT_DIR)
print("- best model:", f"best_model_{best_model_name}.pkl")
print("- model comparison csv:", "model_comparison.csv")
print("- preprocessor:", "preprocessor.pkl")
print("- streamlit app:", "streamlit_app.py")
print("- flask app:", "flask_app.py")

from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [100, 200, 500],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

grid = GridSearchCV(RandomForestClassifier(random_state=RANDOM_STATE),
                    param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid.fit(X_train_pre, y_train)
print("Best params:", grid.best_params_)
print("CV best accuracy:", grid.best_score_)

importances = best_model.feature_importances_
feat_importance = pd.Series(importances, index=feature_cols).sort_values(ascending=False)
feat_importance.plot(kind='bar', figsize=(10,4))


